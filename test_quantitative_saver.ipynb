{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e7efd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "\n",
    "sys.path.append('./src')\n",
    "from dataset import HDMdataset\n",
    "from models import IT2P_history, IT2P_nonhistory\n",
    "from utils import generate_spatial_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9effc6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = json.load(open('./data/dictionary.json', 'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9726a795",
   "metadata": {},
   "source": [
    "### Set below history_flag as True to add history information in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e24fc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_flag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20d61179",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee31d5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "if history_flag:\n",
    "    model = IT2P_history(512, 2, dictionary, 300).to(device)\n",
    "else:\n",
    "    model = IT2P_nonhistory(512, 2, dictionary, 300).to(device)\n",
    "model.eval()\n",
    "\n",
    "spatial_coords = torch.FloatTensor(generate_spatial_batch(1)).permute(0, 3, 1, 2).to(device)\n",
    "print('loaded model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a804409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n",
      "100 3345 412 215 0.00791668621707432\n",
      "loaded model\n",
      "200 3345 796 1082 0.07697471406132349\n",
      "loaded model\n",
      "300 3345 2012 1579 0.2839343016929536\n",
      "loaded model\n",
      "400 3345 2680 2029 0.48598693809335486\n",
      "loaded model\n",
      "500 3345 2750 2074 0.5097405716762631\n",
      "loaded model\n",
      "600 3345 2893 2195 0.5675324704341977\n",
      "loaded model\n",
      "700 3345 2892 2223 0.5745733877616682\n",
      "loaded model\n",
      "800 3345 2920 2228 0.5814411890222785\n",
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100, 1000, 100):\n",
    "    model.load_state_dict(torch.load('./models/history_{}.pth'.format(epoch)))\n",
    "    model.eval()\n",
    "\n",
    "    spatial_coords = torch.FloatTensor(generate_spatial_batch(1)).permute(0, 3, 1, 2).to(device)\n",
    "    print('loaded model')\n",
    "\n",
    "    data_dir = './test_tasks/'\n",
    "    result_dir = './performance/history_{}/'.format(epoch)\n",
    "\n",
    "    pick_correct = 0 \n",
    "    place_correct = 0\n",
    "    pp_cnt = 0\n",
    "    task_correct = 0\n",
    "    task_cnt = 0\n",
    "    thres = 15\n",
    "    for d in sorted(os.listdir(data_dir)):\n",
    "        if d[0] != '.':\n",
    "            img_dir = os.path.join(data_dir, d, 'image')\n",
    "            meta_dir = os.path.join(data_dir, d, 'meta')\n",
    "\n",
    "            os.makedirs(os.path.join(result_dir, d), exist_ok=True)\n",
    "\n",
    "            images = []\n",
    "            for fp in sorted(os.listdir(img_dir)):\n",
    "                img_fp = os.path.join(img_dir, fp)\n",
    "                start_img = transform(Image.open(img_fp))[:3] * 2 - 1\n",
    "                images.append(start_img.unsqueeze(0))\n",
    "\n",
    "            num_pp = len(images)\n",
    "            for fp in sorted(os.listdir(meta_dir)):\n",
    "                meta = json.load(open(os.path.join(meta_dir, fp), 'r'))\n",
    "                dist_results = {'pick':[], 'place':[]}\n",
    "                histories = []\n",
    "\n",
    "                for i in range(num_pp):\n",
    "                    curr_sentence = meta['sentence'][i]\n",
    "                    curr_referential = meta['referential'][i]\n",
    "                    curr_occlusion = meta['visibility'][i]\n",
    "                    gt_bbox = meta['bbox'][i]\n",
    "                    gt_pick = [(gt_bbox['pick'][0] + gt_bbox['pick'][2] ) / 2,\n",
    "                                (gt_bbox['pick'][1] + gt_bbox['pick'][3] ) / 2]\n",
    "                    gt_place = [(gt_bbox['place'][0] + gt_bbox['place'][2] ) / 2,\n",
    "                                 (gt_bbox['place'][1] + gt_bbox['place'][3] ) / 2]\n",
    "\n",
    "                    language = torch.LongTensor([dictionary[w] for w in curr_sentence.split()]).unsqueeze(0)\n",
    "                    time = torch.LongTensor([i]).to(device)\n",
    "                    lang_lengths = torch.LongTensor([len(curr_sentence.split())])\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        if history_flag:\n",
    "                            pred, histories = model(images[i].float().to(device), \n",
    "                                                    language.long().to(device),\n",
    "                                                    lang_lengths, spatial_coords, time, histories)\n",
    "                        else:\n",
    "                            pred = model(images[i].float().to(device), \n",
    "                                         language.long().to(device),\n",
    "                                         lang_lengths, \n",
    "                                         spatial_coords)\n",
    "                        \n",
    "                        \n",
    "                    pick_pred = resize(pred[:, 0, :, :].squeeze().detach().cpu().numpy(), (256, 256))\n",
    "                    place_pred = resize(pred[:, 1, :, :].squeeze().detach().cpu().numpy(), (256, 256))\n",
    "\n",
    "                    esti_pick = np.where(pick_pred == np.max(pick_pred))\n",
    "                    esti_pick = [esti_pick[1][0], esti_pick[0][0]]\n",
    "                    esti_place = np.where(place_pred == np.max(place_pred))\n",
    "                    esti_place = [esti_place[1][0], esti_place[0][0]]\n",
    "\n",
    "                    pick_dist = np.sqrt((gt_pick[0]-esti_pick[0]) ** 2 + (gt_pick[1]-esti_pick[1])**2)\n",
    "                    place_dist = np.sqrt((gt_place[0]-esti_place[0]) ** 2 + (gt_place[1]-esti_place[1])**2)\n",
    "\n",
    "                    dist_results['pick'].append([int(esti_pick[0]), int(esti_pick[1])])\n",
    "                    dist_results['place'].append([int(esti_place[0]), int(esti_place[1])])\n",
    "\n",
    "                    if pick_dist < thres:\n",
    "                        pick_correct += 1\n",
    "                    if place_dist < thres:\n",
    "                        place_correct += 1\n",
    "                    pp_cnt += 1\n",
    "                del histories\n",
    "                histories = []\n",
    "                torch.cuda.empty_cache()\n",
    "                json.dump(dist_results, open(os.path.join(result_dir, d, fp), 'w' ))\n",
    "    print(epoch, pp_cnt, pick_correct, place_correct, (pick_correct/pp_cnt) * (place_correct/pp_cnt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
